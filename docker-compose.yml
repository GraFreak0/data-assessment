services:
  airflow:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        AIRFLOW_VERSION: "2.3"
        SPARK_VERSION: "3.3.0"
        HADOOP_VERSION: "3.3.2"
        SCALA_VERSION: "2.12"
        JAVA_VERSION: "11"
        PYTHON_VERSION: "3.9"
        SQLALCHEMY_VERSION: "1.4"
    ports:
      - 8080:8080
    environment:
      # This uses sqlite as database by default
      ENABLE_AIRFLOW_INITDB: "true"
      AIRFLOW__CORE__FERNET_KEY: 8NE6O6RcTJpxcCkuKxOHOExzIJkXkeJKbRie03a69dI=
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__SCHEDULER__MAX_THREADS: "1"
      ENABLE_AIRFLOW_RBAC_SETUP_AUTH: "true"
      AIRFLOW_WEBSERVER_RBAC_USER: admin
      AIRFLOW_WEBSERVER_RBAC_PASSWORD: Password123
      AIRFLOW_WEBSERVER_RBAC_EMAIL: admin-san@xyz.com
      AIRFLOW_WEBSERVER_RBAC_FIRST_NAME: admin
      AIRFLOW_WEBSERVER_RBAC_LAST_NAME: san
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: sqlite:////airflow/airflow.db
    volumes:
        - ${PWD}/dags:/airflow/dags
        - airflow-data:/airflow
    env_file:
        - .env
    restart: always
    networks:
            - ghost


  # Jupyter notebook
  jupyter-spark:
        hostname: myjupyter
        container_name: jupyter_container
        image: 'jupyter/pyspark-notebook:spark-3.2.0'
        networks:
            - ghost
        ports:
          - "8888:8888"
        volumes:
          - ./notebooks:/home/jovyan/work/notebooks/
          - airflow-data:/home/jovyan/airflow
        restart: always
          



  

# Network Bridge Connection
volumes:
  airflow-data:


# Network Bridge Connection
networks:
  ghost:
    driver: bridge